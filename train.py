import torch
import torch.nn as nn
from transformers import BartForConditionalGeneration, AdamW, get_scheduler
from tqdm.auto import tqdm

# ===================== 1. åŸºç¡€é…ç½®ï¼ˆå…¨éƒ¨è°ƒå¥½ï¼Œä¸ç”¨æ”¹ï¼‰ =====================
device = "cuda" if torch.cuda.is_available() else "cpu"  # è‡ªåŠ¨ç”¨GPU/CPU
model_name = "fnlp/bart-base-chinese"
epochs = 15  # è®­ç»ƒè½®æ•°ï¼Œ15è½®è¶³å¤Ÿå‡ºå¥½æ•ˆæœï¼Œå¤šäº†ä¼šè¿‡æ‹Ÿåˆ
lr = 2e-5    # å­¦ä¹ ç‡ï¼ŒBARTæœ€ä¼˜å­¦ä¹ ç‡ï¼Œä¸ç”¨è°ƒ

# ===================== 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ =====================
model = BartForConditionalGeneration.from_pretrained(model_name).to(device)
tokenizer = BartTokenizer.from_pretrained(model_name)

# ===================== 3. å®šä¹‰ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ =====================
optimizer = AdamW(model.parameters(), lr=lr)
num_training_steps = epochs * len(train_spoken)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

# ===================== 4. è®­ç»ƒå‡½æ•°ï¼ˆæ ¸å¿ƒï¼Œè‡ªåŠ¨è®­ç»ƒ+éªŒè¯ï¼‰ =====================
def train_epoch(model, inputs, labels, optimizer, scheduler, device):
    model.train()
    total_loss = 0
    # æŠŠæ•°æ®ç§»åˆ°GPU/CPU
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)
    labels = labels.to(device)
    
    # å‰å‘ä¼ æ’­+è®¡ç®—æŸå¤±+åå‘ä¼ æ’­
    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    loss = outputs.loss
    total_loss += loss.item()
    
    loss.backward()
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()
    
    return total_loss / len(train_spoken)

def val_epoch(model, inputs, labels, device):
    model.eval()
    total_loss = 0
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)
    labels = labels.to(device)
    
    with torch.no_grad():  # éªŒè¯æ—¶ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœç®—åŠ›
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
    
    return total_loss / len(val_spoken)

# ===================== 5. å¼€å§‹è®­ç»ƒï¼ˆä¸€é”®è¿è¡Œï¼Œè‡ªåŠ¨æ‰“å°æ—¥å¿—ï¼‰ =====================
progress_bar = tqdm(range(num_training_steps))
best_val_loss = float("inf")  # ä¿å­˜æœ€ä¼˜æ¨¡å‹çš„éªŒè¯æŸå¤±

for epoch in range(epochs):
    train_loss = train_epoch(model, train_inputs, train_labels, optimizer, lr_scheduler, device)
    val_loss = val_epoch(model, val_inputs, val_labels, device)
    
    # æ‰“å°æ¯è½®è®­ç»ƒç»“æœ
    print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
    progress_bar.update(len(train_spoken))
    
    # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆæŸå¤±æœ€ä½çš„æ¨¡å‹ï¼Œæ•ˆæœæœ€å¥½ï¼‰
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        model.save_pretrained("./best_bart_style_transfer")
        tokenizer.save_pretrained("./best_bart_style_transfer")
        print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼Œå½“å‰æœ€ä¼˜éªŒè¯æŸå¤±ï¼š{best_val_loss:.4f}")

print("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜åˆ° ./best_bart_style_transfer æ–‡ä»¶å¤¹")